{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19d5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "lang_code = {\n",
    "    \"asturiano\": {\n",
    "        \"tatoeba\": \"ast\",\n",
    "        \"opus\": \"ast\"\n",
    "    },\n",
    "    \"aranes\": {\n",
    "        \"tatoeba\": \"oci\",\n",
    "        \"opus\": \"oc\"\n",
    "    },\n",
    "    \"aragones\": {\n",
    "        \"tatoeba\": \"arg\",\n",
    "        \"opus\": \"an\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDatasets():\n",
    "    def __init__(self, language, initialize=True):\n",
    "        if language not in [\"aragones\", \"asturiano\", \"occitano\"]:\n",
    "            raise KeyError(\"Lenguaje no contemplado\")\n",
    "        self.raw_datasets = {}\n",
    "        self.language = language\n",
    "        self.language_codes = lang_code[language]\n",
    "        self.json = []\n",
    "        if initialize:\n",
    "            self.start()\n",
    "\n",
    "    @property\n",
    "    def hf_dataset(self):\n",
    "        \"\"\"Devuelve un Dataset de HuggingFace a partir de self.json\"\"\"\n",
    "        return Dataset.from_list(self.json)\n",
    "\n",
    "    def tokenize(self, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Aplica un tokenizer externo al dataset.\n",
    "        Devuelve un HuggingFace Dataset tokenizado listo para entrenamiento.\n",
    "        \"\"\"\n",
    "            # Aseguramos que el tokenizer tenga pad_token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        def _tokenize(example):\n",
    "            result = tokenizer(\n",
    "                example[\"text\"],\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "            return result\n",
    "\n",
    "        return self.hf_dataset.map(_tokenize)\n",
    "\n",
    "    def start(self):\n",
    "        print(f\"Descargando tatoeba para {self.language}:\")\n",
    "        try:\n",
    "            self.read_tatoeba_url(\n",
    "                f\"https://downloads.tatoeba.org/exports/per_language/\"\n",
    "                f\"{self.language_codes['tatoeba']}/\"\n",
    "                f\"{self.language_codes['tatoeba']}_sentences_detailed.tsv.bz2\"\n",
    "            )mistralai/Mistral-7B-v0.1\n",
    "            print(\"Completado con éxito\")\n",
    "        except Exception as e:\n",
    "            print(\"No se pudo completar por:\", e)\n",
    "\n",
    "        print(f\"Cargando txt locales para {self.language}:\")\n",
    "        self.read_folder(f\"datasets/{self.language}\")\n",
    "\n",
    "    def read_tatoeba_url(self, url):\n",
    "        df = pd.read_csv(\n",
    "            url,\n",
    "            sep=\"\\t\",\n",
    "            compression=\"bz2\",\n",
    "            header=None,\n",
    "            names=[\"id\", \"lang\", \"text\", \"author\", \"created_at\", \"updated_at\"]\n",
    "        )\n",
    "        if df.iloc[0][\"lang\"] != self.language_codes[\"tatoeba\"]:\n",
    "            raise ValueError(\"El dataset descargado no corresponde al idioma esperado\")\n",
    "        elif \"tatoeba\" in self.raw_datasets:\n",
    "            raise ValueError(\"El dataset tatoeba ya está cargado\")\n",
    "\n",
    "        json_data = self.pandas_to_json(df)\n",
    "        start = len(self.json)\n",
    "        self.json += json_data\n",
    "        end = len(self.json)\n",
    "        self.raw_datasets[\"tatoeba\"] = {\"start\": start, \"end\": end}\n",
    "        return df\n",
    "\n",
    "    def read_folder(self, directory):\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith(\".txt\"):\n",
    "                try:\n",
    "                    self.read_local_file(directory, file)\n",
    "                    print(f\"Archivo {file} cargado\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Fallo al cargar el archivo {file}: {e}\")\n",
    "\n",
    "    def read_local_file(self, directory, file):\n",
    "        dataset_name = file.split(\".\")[0]\n",
    "        if dataset_name in self.raw_datasets.keys():\n",
    "            raise ValueError(f\"El dataset {directory}/{file} ya está cargado\")\n",
    "\n",
    "        json_data = []\n",
    "        with open(os.path.join(directory, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                clean_line = self.clean_text(line)\n",
    "                if clean_line:\n",
    "                    json_data.append({\"text\": clean_line})\n",
    "\n",
    "        start = len(self.json)\n",
    "        self.json += json_data\n",
    "        end = len(self.json)\n",
    "        self.raw_datasets[dataset_name] = {\"start\": start, \"end\": end}\n",
    "        return json_data\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r\"[^\\w\\s\\n]\", \" \", text)\n",
    "        text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "        text = re.sub(r\"(.)\\1{5,}\", r\"\\1\"*5, text)\n",
    "        def limit_word_reps(match):\n",
    "            word = match.group(1)\n",
    "            return \" \".join([word]*5)\n",
    "        text = re.sub(r\"\\b(\\w+)( \\1){5,}\\b\", limit_word_reps, text)\n",
    "        text = text.lower()\n",
    "        return text.strip()\n",
    "\n",
    "    def pandas_to_json(self, df, clean=True, save=False):\n",
    "        json_data = []\n",
    "        for t in df[\"text\"].tolist():\n",
    "            clean_line = self.clean_text(t) if clean else t\n",
    "            if clean_line:\n",
    "                json_data.append({\"text\": clean_line})\n",
    "        if save:\n",
    "            with open(save, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(json_data, ensure_ascii=False))\n",
    "        return json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 12:30:14.669076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando tatoeba para asturiano:\n",
      "Completado con éxito\n",
      "Cargando txt locales para asturiano:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49840e46a86642019109aab0b681a57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e2cf40a3bc4b6e92aa326a6df078a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1deb0b56243f4e9390bf7ef5f1982601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58eff8b9bcce485d983e978b948d8a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7107fbdd4b8f46bcb75c69d356abc9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea16ebfde362482fa07c825d960412ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# Inicializamos dataset\n",
    "ast = LanguageDatasets(\"asturiano\")\n",
    "\n",
    "# Cargamos tokenizer de Mistral\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenizamos dataset\n",
    "tokenized_dataset = ast.tokenize(tokenizer)\n",
    "\n",
    "# Ajustamos labels para ignorar el relleno en la pérdida\n",
    "def mask_labels(example):\n",
    "    example[\"labels\"] = [\n",
    "        (id if mask == 1 else -100)\n",
    "        for id, mask in zip(example[\"input_ids\"], example[\"attention_mask\"])\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(mask_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4bc2d",
   "metadata": {},
   "source": [
    "### Preparar modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b54a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41e8c860a8142ec843952dfc402b7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/683 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fed3e84745423a8dfb72e562df8c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e2391f1ff944cb9e4cd74d7a87764f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bd7c6a895741d396f52da18612bb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b922d9c9634972b2b67680fae133f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb1b6040833495da00fb44915a86130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error named symbol not found at line 57 in file /src/csrc/ops.cu\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06c882",
   "metadata": {},
   "source": [
    "### Configurar Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],  # módulos típicos en Mistral/LLaMA\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ccac5",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c13920",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca58dc",
   "metadata": {},
   "source": [
    "## Usar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Ruta donde guardaste tu LoRA tras el entrenamiento\n",
    "lora_path = \"./lora-asturiano\"\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"¿Cómo ta el cielu al atapecer en Xixón?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
