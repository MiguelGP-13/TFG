{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b102389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "GPU: NVIDIA GeForce GTX 970\n",
      "Memoria disponible: 4.22 GB\n",
      "Número de GPUs visibles: 1\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# IMPORTANTE: Configurar para usar solo 1 GPU ANTES de importar transformers\n",
    "# Esto previene errores NCCL en sistemas con múltiples GPUs\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Para debugging si es necesario\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verificar GPU\n",
    "device = \"cpu\"#\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoria disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Número de GPUs visibles: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e19d5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lang_code = {\n",
    "    \"asturiano\": {\n",
    "        \"tatoeba\": \"ast\",\n",
    "        \"opus\": \"ast\"\n",
    "    },\n",
    "    \"aranes\": {\n",
    "        \"tatoeba\": \"oci\",\n",
    "        \"opus\": \"oc\"\n",
    "    },\n",
    "    \"aragones\": {\n",
    "        \"tatoeba\": \"arg\",\n",
    "        \"opus\": \"an\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f691276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDatasets():\n",
    "    def __init__(self, language, initialize=True):\n",
    "        if language not in [\"aragones\", \"asturiano\", \"occitano\"]:\n",
    "            raise KeyError(\"Lenguaje no contemplado\")\n",
    "        self.raw_datasets = {}\n",
    "        self.language = language\n",
    "        self.language_codes = lang_code[language]\n",
    "        self.json = []\n",
    "        if initialize:\n",
    "            self.start()\n",
    "\n",
    "    @property\n",
    "    def hf_dataset(self):\n",
    "        \"\"\"Devuelve un Dataset de HuggingFace a partir de self.json\"\"\"\n",
    "        return Dataset.from_list(self.json)\n",
    "\n",
    "    def tokenize(self, tokenizer, max_length=512):\n",
    "        \"\"\"\n",
    "        Aplica un tokenizer externo al dataset.\n",
    "        Devuelve un HuggingFace Dataset tokenizado listo para entrenamiento.\n",
    "        \"\"\"\n",
    "            # Aseguramos que el tokenizer tenga pad_token\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        def _tokenize(example):\n",
    "            result = tokenizer(\n",
    "                example[\"text\"],\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "            return result\n",
    "\n",
    "        return self.hf_dataset.map(_tokenize)\n",
    "\n",
    "    def start(self):\n",
    "        print(f\"Descargando tatoeba para {self.language}:\")\n",
    "        try:\n",
    "            self.read_tatoeba_url(\n",
    "                f\"https://downloads.tatoeba.org/exports/per_language/\"\n",
    "                f\"{self.language_codes['tatoeba']}/\"\n",
    "                f\"{self.language_codes['tatoeba']}_sentences_detailed.tsv.bz2\"\n",
    "            )\n",
    "            print(\"Completado con éxito\")\n",
    "        except Exception as e:\n",
    "            print(\"No se pudo completar por:\", e)\n",
    "\n",
    "        print(f\"Cargando txt locales para {self.language}:\")\n",
    "        self.read_folder(f\"datasets/{self.language}\")\n",
    "\n",
    "    def read_tatoeba_url(self, url):\n",
    "        df = pd.read_csv(\n",
    "            url,\n",
    "            sep=\"\\t\",\n",
    "            compression=\"bz2\",\n",
    "            header=None,\n",
    "            names=[\"id\", \"lang\", \"text\", \"author\", \"created_at\", \"updated_at\"]\n",
    "        )\n",
    "        if df.iloc[0][\"lang\"] != self.language_codes[\"tatoeba\"]:\n",
    "            raise ValueError(\"El dataset descargado no corresponde al idioma esperado\")\n",
    "        elif \"tatoeba\" in self.raw_datasets:\n",
    "            raise ValueError(\"El dataset tatoeba ya está cargado\")\n",
    "\n",
    "        json_data = self.pandas_to_json(df)\n",
    "        start = len(self.json)\n",
    "        self.json += json_data\n",
    "        end = len(self.json)\n",
    "        self.raw_datasets[\"tatoeba\"] = {\"start\": start, \"end\": end}\n",
    "        return df\n",
    "\n",
    "    def read_folder(self, directory):\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith(\".txt\"):\n",
    "                try:\n",
    "                    self.read_local_file(directory, file)\n",
    "                    print(f\"Archivo {file} cargado\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Fallo al cargar el archivo {file}: {e}\")\n",
    "\n",
    "    def read_local_file(self, directory, file):\n",
    "        dataset_name = file.split(\".\")[0]\n",
    "        if dataset_name in self.raw_datasets.keys():\n",
    "            raise ValueError(f\"El dataset {directory}/{file} ya está cargado\")\n",
    "\n",
    "        json_data = []\n",
    "        with open(os.path.join(directory, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                clean_line = self.clean_text(line)\n",
    "                if clean_line:\n",
    "                    json_data.append({\"text\": clean_line})\n",
    "\n",
    "        start = len(self.json)\n",
    "        self.json += json_data\n",
    "        end = len(self.json)\n",
    "        self.raw_datasets[dataset_name] = {\"start\": start, \"end\": end}\n",
    "        return json_data\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r\"[^\\w\\s\\n]\", \" \", text)\n",
    "        text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "        text = re.sub(r\"(.)\\1{5,}\", r\"\\1\"*5, text)\n",
    "        def limit_word_reps(match):\n",
    "            word = match.group(1)\n",
    "            return \" \".join([word]*5)\n",
    "        text = re.sub(r\"\\b(\\w+)( \\1){5,}\\b\", limit_word_reps, text)\n",
    "        text = text.lower()\n",
    "        return text.strip()\n",
    "\n",
    "    def pandas_to_json(self, df, clean=True, save=False):\n",
    "        json_data = []\n",
    "        for t in df[\"text\"].tolist():\n",
    "            clean_line = self.clean_text(t) if clean else t\n",
    "            if clean_line:\n",
    "                json_data.append({\"text\": clean_line})\n",
    "        if save:\n",
    "            with open(save, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(json_data, ensure_ascii=False))\n",
    "        return json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f67f797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando tatoeba para asturiano:\n",
      "Completado con éxito\n",
      "Cargando txt locales para asturiano:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed2b44dd7ba41ebb392b706c1c34824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e7af6b73e54f5686c353573dd9d514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# Inicializamos dataset\n",
    "ast = LanguageDatasets(\"asturiano\")\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenizamos dataset\n",
    "tokenized_dataset = ast.tokenize(tokenizer)\n",
    "\n",
    "# Ajustamos labels para ignorar el relleno en la pérdida\n",
    "def mask_labels(example):\n",
    "    example[\"labels\"] = [\n",
    "        (id if mask == 1 else -100)\n",
    "        for id, mask in zip(example[\"input_ids\"], example[\"attention_mask\"])\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.map(mask_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63b454",
   "metadata": {},
   "source": [
    "### Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2113e36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator para padding dinámico\n",
    "# Alternativa a padding=\"max_length\": permite diferentes longitudes por batch\n",
    "# Más eficiente en memoria cuando las secuencias tienen longitudes variadas\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,     # Tokenizer para aplicar padding\n",
    "    padding=True,            # Aplicar padding dinámico\n",
    "    max_length=None,         # Sin límite adicional (usa el del tokenizer)\n",
    "    pad_to_multiple_of=None  # Sin redondeo de longitud\n",
    ")\n",
    "\n",
    "# Métrica de evaluación\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calcula métricas de evaluación durante el entrenamiento.\n",
    "    \n",
    "    Args:\n",
    "        eval_pred: Tuple con (predictions, labels)\n",
    "                   predictions: logits del modelo (shape: [batch_size, num_labels])\n",
    "                   labels: etiquetas verdaderas (shape: [batch_size])\n",
    "    \n",
    "    Returns:\n",
    "        Dict con métricas calculadas\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # Convertir logits a clases predichas (argmax sobre dimensión de clases)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    # Calcular accuracy comparando predicciones con labels verdaderos\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4bc2d",
   "metadata": {},
   "source": [
    "### Preparar modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd1b54a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08bc5b5771e41ffa6293ead7db29086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\"   # usa GPU si está disponible\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06c882",
   "metadata": {},
   "source": [
    "### Configurar Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "831e4289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Información del modelo con LoRA:\n",
      "trainable params: 3,686,400 || all params: 3,089,625,088 || trainable%: 0.1193\n",
      "\n",
      "RESUMEN:\n",
      "Parámetros totales: 3,089,625,088\n",
      "Parámetros entrenables: 3,686,400\n",
      "Porcentaje entrenable: 0.12%\n",
      "\n",
      "¡Solo entrenamos ~1% de los parámetros del modelo!\n",
      "\n",
      "Con rank r=8:\n",
      "  - Cada matriz LoRA añade: d×r + r×k parámetros\n",
      "  - Para attention de dim 768: ~24,576 params por capa\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # módulos típicos en Qwen/Mistral\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Aplicar LoRA al modelo\n",
    "model_lora = get_peft_model(model, lora_config)\n",
    "\n",
    "# Imprimir información del modelo\n",
    "print(\"\\nInformación del modelo con LoRA:\")\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "# Contar parámetros manualmente para comparación\n",
    "total_params_lora = sum(p.numel() for p in model_lora.parameters())\n",
    "trainable_params_lora = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nRESUMEN:\")\n",
    "print(f\"Parámetros totales: {total_params_lora:,}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params_lora:,}\")\n",
    "print(f\"Porcentaje entrenable: {100 * trainable_params_lora / total_params_lora:.2f}%\")\n",
    "print(f\"\\n¡Solo entrenamos ~1% de los parámetros del modelo!\")\n",
    "print(f\"\\nCon rank r={lora_config.r}:\")\n",
    "print(f\"  - Cada matriz LoRA añade: d×r + r×k parámetros\")\n",
    "print(f\"  - Para attention de dim 768: ~{(768*8 + 8*768)*2:,} params por capa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ccac5",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87c13920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "# Configuración de entrenamiento\n",
    "training_args_lora = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    num_train_epochs=3,              # Número de épocas completas de entrenamiento\n",
    "    per_device_train_batch_size=8,   # Batch size para entrenamiento\n",
    "    per_device_eval_batch_size=16,   # Batch size para evaluación (puede ser mayor)\n",
    "    learning_rate=2e-4,              # Learning rate (típicamente más alto con LoRA: 1e-4 a 3e-4)\n",
    "    weight_decay=0.01,               # Regularización L2\n",
    "    eval_strategy=\"no\",           # Evaluar al final de cada época\n",
    "    save_strategy=\"no\",              # No guardar checkpoints (para rapidez)\n",
    "    logging_steps=50,                # Log cada 50 steps\n",
    "    report_to=\"none\",                # No reportar a wandb/tensorboard\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision training si hay GPU\n",
    ")\n",
    "\n",
    "# Crear Trainer\n",
    "trainer_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43168e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria GPU inicial: 0.00 GB\n",
      "\n",
      "Iniciando entrenamiento con LoRA...\n",
      "   (Esto debería ser más rápido que full fine-tuning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenar y medir tiempo/memoria\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Métricas de memoria\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_memory_lora = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "    print(f\"Memoria GPU inicial: {start_memory_lora:.2f} GB\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\nIniciando entrenamiento con LoRA...\")\n",
    "print(\"   (Esto debería ser más rápido que full fine-tuning)\\n\")\n",
    "\n",
    "trainer_lora.train()\n",
    "\n",
    "end_time = time.time()\n",
    "training_time_lora = end_time - start_time\n",
    "\n",
    "# Reportar uso de memoria\n",
    "if torch.cuda.is_available():\n",
    "    peak_memory_lora = torch.cuda.max_memory_allocated() / 1e9  # GB\n",
    "    current_memory_lora = torch.cuda.memory_allocated() / 1e9  # GB\n",
    "    print(f\"\\nREPORTE DE MEMORIA:\")\n",
    "    print(f\"   Memoria inicial: {start_memory_lora:.2f} GB\")\n",
    "    print(f\"   Memoria actual: {current_memory_lora:.2f} GB\")\n",
    "    print(f\"   Memoria pico durante entrenamiento: {peak_memory_lora:.2f} GB\")\n",
    "    print(f\"   Memoria adicional usada: {peak_memory_lora - start_memory_lora:.2f} GB\")\n",
    "else:\n",
    "    peak_memory_lora = 0\n",
    "    print(f\"\\nEjecutando en CPU (no hay métricas de memoria GPU)\")\n",
    "\n",
    "print(f\"\\nTIEMPO DE ENTRENAMIENTO:\")\n",
    "print(f\"   {training_time_lora:.2f} segundos ({training_time_lora/60:.2f} minutos)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd5084",
   "metadata": {},
   "source": [
    "### Evaluar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar\n",
    "print(\"\\nEvaluando modelo con LoRA...\")\n",
    "eval_results_lora = trainer_lora.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESULTADOS - LORA (PEFT)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy: {eval_results_lora['eval_accuracy']:.4f}\")\n",
    "print(f\"Loss: {eval_results_lora['eval_loss']:.4f}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params_lora:,} ({100 * trainable_params_lora / total_params_lora:.2f}%)\")\n",
    "print(f\"Tiempo: {training_time_lora:.2f}s ({training_time_lora/60:.2f} min)\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memoria pico: {peak_memory_lora:.2f} GB\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nLoRA permite hacer fine-tuning de modelos grandes eficientemente\")\n",
    "print(\"   Ideal para: recursos limitados, múltiples tareas, experimentación rápida\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca58dc",
   "metadata": {},
   "source": [
    "## Usar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cf1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Ruta donde guardaste tu LoRA tras el entrenamiento\n",
    "lora_path = \"./lora-asturiano\"\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"¿Cómo ta el cielu al atapecer en Xixón?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
